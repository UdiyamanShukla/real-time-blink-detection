{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f478d71e-42a4-4824-982c-d6ea6fe43c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import warnings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91ad0bca-160b-4e12-8980-1619bd1126af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BLINK] 046: 100%|█████████████████████████████| 17/17 [00:00<00:00, 814.99it/s]\n",
      "[BLINK] 047: 100%|██████████████████████████████| 99/99 [05:20<00:00,  3.24s/it]\n",
      "[BLINK] 048: 100%|██████████████████████████████| 57/57 [03:06<00:00,  3.28s/it]\n",
      "[BLINK] 049: 100%|██████████████████████████████| 66/66 [03:41<00:00,  3.35s/it]\n",
      "[BLINK] 050: 100%|██████████████████████████████| 15/15 [00:51<00:00,  3.45s/it]\n",
      "[BLINK] 051: 100%|██████████████████████████████| 55/55 [03:07<00:00,  3.41s/it]\n",
      "[BLINK] 052: 100%|██████████████████████████████| 82/82 [04:36<00:00,  3.38s/it]\n",
      "[BLINK] 053: 100%|██████████████████████████████| 41/41 [02:19<00:00,  3.40s/it]\n",
      "[BLINK] 054: 100%|██████████████████████████████| 74/74 [04:10<00:00,  3.38s/it]\n",
      "[BLINK] 055: 100%|█████████████████████████████| 55/55 [00:00<00:00, 862.31it/s]\n",
      "[BLINK] 056: 100%|██████████████████████████████| 78/78 [04:26<00:00,  3.41s/it]\n",
      "[BLINK] 057: 100%|██████████████████████████████| 26/26 [01:28<00:00,  3.40s/it]\n",
      "[BLINK] 058: 100%|██████████████████████████████| 93/93 [05:16<00:00,  3.40s/it]\n",
      "[BLINK] 060: 100%|██████████████████████████████| 62/62 [03:35<00:00,  3.48s/it]\n",
      "[BLINK] 061: 100%|█████████████████████████████| 56/56 [00:00<00:00, 847.59it/s]\n",
      "[BLINK] 062: 100%|██████████████████████████████| 89/89 [05:03<00:00,  3.41s/it]\n",
      "[BLINK] 063: 100%|█████████████████████████████| 42/42 [00:00<00:00, 820.59it/s]\n",
      "[BLINK] 064: 100%|██████████████████████████████| 79/79 [04:37<00:00,  3.51s/it]\n",
      "[BLINK] 065: 100%|██████████████████████████████| 67/67 [03:48<00:00,  3.42s/it]\n",
      "[BLINK] 066: 100%|██████████████████████████████| 23/23 [01:02<00:00,  2.72s/it]\n",
      "[BLINK] 068: 100%|███████████████████████████| 126/126 [00:00<00:00, 905.99it/s]\n",
      "[BLINK] 069: 100%|██████████████████████████████| 28/28 [01:37<00:00,  3.47s/it]\n",
      "[BLINK] 070: 100%|██████████████████████████████| 63/63 [03:40<00:00,  3.50s/it]\n",
      "[BLINK] 071: 100%|██████████████████████████████| 42/42 [02:24<00:00,  3.45s/it]\n",
      "[BLINK] 072: 100%|█████████████████████████████| 24/24 [00:00<00:00, 830.34it/s]\n",
      "[BLINK] 073: 100%|█████████████████████████████| 76/76 [00:00<00:00, 874.34it/s]\n",
      "[BLINK] 074: 100%|█████████████████████████████| 56/56 [00:00<00:00, 878.67it/s]\n",
      "[BLINK] 075: 100%|███████████████████████████| 215/215 [00:00<00:00, 900.72it/s]\n",
      "[BLINK] 076: 100%|█████████████████████████████| 77/77 [00:00<00:00, 893.60it/s]\n",
      "[BLINK] 077: 100%|█████████████████████████████| 70/70 [00:00<00:00, 883.30it/s]\n",
      "[OPEN] 046: 100%|██████████████████████████████| 54/54 [00:00<00:00, 922.44it/s]\n",
      "[OPEN] 047: 100%|███████████████████████████████| 83/83 [04:40<00:00,  3.38s/it]\n",
      "[OPEN] 048: 100%|███████████████████████████████| 99/99 [05:42<00:00,  3.46s/it]\n",
      "[OPEN] 049: 100%|███████████████████████████████| 82/82 [04:43<00:00,  3.46s/it]\n",
      "[OPEN] 050: 100%|███████████████████████████████| 61/61 [03:27<00:00,  3.40s/it]\n",
      "[OPEN] 051: 100%|███████████████████████████████| 78/78 [04:29<00:00,  3.45s/it]\n",
      "[OPEN] 052: 100%|███████████████████████████████| 84/84 [04:44<00:00,  3.38s/it]\n",
      "[OPEN] 053: 100%|███████████████████████████████| 37/37 [02:05<00:00,  3.39s/it]\n",
      "[OPEN] 054: 100%|███████████████████████████████| 91/91 [05:03<00:00,  3.33s/it]\n",
      "[OPEN] 055: 100%|██████████████████████████████| 46/46 [00:00<00:00, 848.58it/s]\n",
      "[OPEN] 056: 100%|███████████████████████████████| 58/58 [03:15<00:00,  3.38s/it]\n",
      "[OPEN] 057: 100%|███████████████████████████████| 69/69 [03:49<00:00,  3.33s/it]\n",
      "[OPEN] 058: 100%|█████████████████████████████| 139/139 [07:47<00:00,  3.36s/it]\n",
      "[OPEN] 060: 100%|███████████████████████████████| 92/92 [05:07<00:00,  3.34s/it]\n",
      "[OPEN] 061: 100%|██████████████████████████████| 71/71 [00:00<00:00, 864.13it/s]\n",
      "[OPEN] 062: 100%|███████████████████████████████| 72/72 [04:00<00:00,  3.34s/it]\n",
      "[OPEN] 063: 100%|██████████████████████████████| 43/43 [00:00<00:00, 824.88it/s]\n",
      "[OPEN] 064: 100%|███████████████████████████████| 88/88 [04:49<00:00,  3.29s/it]\n",
      "[OPEN] 065: 100%|█████████████████████████████| 133/133 [07:09<00:00,  3.23s/it]\n",
      "[OPEN] 066: 100%|█████████████████████████████| 144/144 [06:22<00:00,  2.66s/it]\n",
      "[OPEN] 068: 100%|████████████████████████████| 124/124 [00:00<00:00, 872.44it/s]\n",
      "[OPEN] 069: 100%|█████████████████████████████| 201/201 [10:52<00:00,  3.25s/it]\n",
      "[OPEN] 070: 100%|█████████████████████████████| 164/164 [08:59<00:00,  3.29s/it]\n",
      "[OPEN] 071: 100%|███████████████████████████████| 55/55 [03:00<00:00,  3.29s/it]\n",
      "[OPEN] 072: 100%|████████████████████████████| 103/103 [00:00<00:00, 832.50it/s]\n",
      "[OPEN] 073: 100%|████████████████████████████| 111/111 [00:00<00:00, 880.48it/s]\n",
      "[OPEN] 074: 100%|██████████████████████████████| 42/42 [00:00<00:00, 858.84it/s]\n",
      "[OPEN] 075: 100%|████████████████████████████| 201/201 [00:00<00:00, 874.46it/s]\n",
      "[OPEN] 076: 100%|████████████████████████████| 109/109 [00:00<00:00, 876.33it/s]\n",
      "[OPEN] 077: 100%|██████████████████████████████| 93/93 [00:00<00:00, 864.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2938 successful results to eye_segmentation_results_contour_analysis_complete_with_no_dummy_tuned_tester3.csv\n",
      "Saved 1842 errors to processing_errors_with_no_dummy_tuned_tester3.csv\n",
      "Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "# Initialize SAM2\n",
    "checkpoint = \"/Users/udiyamanshukla_1/Desktop/C_Drive/MSc_project/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint, device=\"cpu\"))\n",
    "\n",
    "# Configuration\n",
    "consolidated_csv = 'consolidated_coordinates.csv'  # Changed to single CSV file\n",
    "image_dirs = {'blink': 'Blink_tester_3', 'open': 'Open_tester_3'}\n",
    "output_csv = \"eye_segmentation_results_contour_analysis_complete_with_no_dummy_tuned_tester3.csv\"\n",
    "error_csv = \"processing_errors_with_no_dummy_tuned_tester3.csv\"\n",
    "\n",
    "# Data storage\n",
    "results = []\n",
    "error_log = []\n",
    "\n",
    "# Updated function. Now look at cases where masks > 2 as well\n",
    "def has_two_non_overlapping_regions(mask, min_size_ratio=0.3): # Was 0.5 earlier, now modified to 0.3\n",
    "    \"\"\"\n",
    "    Check if mask has at least two non-overlapping regions where the largest two\n",
    "    are roughly the same size.\n",
    "    \n",
    "    Args:\n",
    "        mask (ndarray): Binary mask image\n",
    "        min_size_ratio (float): Minimum ratio between smaller and larger region (default: 0.5)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the two largest regions meet the size ratio criteria\n",
    "    \"\"\"\n",
    "    labeled_mask, num_features = ndimage.label(mask)\n",
    "    \n",
    "    # Need at least 2 regions to compare\n",
    "    if num_features < 2:\n",
    "        return False\n",
    "    \n",
    "    # Calculate sizes of all regions\n",
    "    sizes = [np.sum(labeled_mask == i) for i in range(1, num_features + 1)]\n",
    "    \n",
    "    # Get indices of the two largest regions\n",
    "    largest_two_indices = np.argsort(sizes)[-2:]\n",
    "    size1, size2 = sizes[largest_two_indices[0]], sizes[largest_two_indices[1]]\n",
    "    \n",
    "    # Calculate size ratio (smaller/larger)\n",
    "    size_ratio = min(size1, size2) / max(size1, size2)\n",
    "    \n",
    "    return size_ratio >= min_size_ratio\n",
    "\n",
    "\n",
    "def calculate_eccentricity(mask):\n",
    "    \"\"\"Calculate eccentricity of the second largest non-overlapping region.\n",
    "       Returns 1 if region is not valid or fitting fails.\"\"\"\n",
    "    labeled_mask, num_features = ndimage.label(mask)\n",
    "\n",
    "    if num_features < 2:\n",
    "        return 1\n",
    "\n",
    "    # Get second largest region\n",
    "    region_areas = [(i, np.sum(labeled_mask == i)) for i in range(1, num_features + 1)]\n",
    "    region_areas.sort(key=lambda x: x[1], reverse=True)\n",
    "    second_largest_label = region_areas[1][0]\n",
    "\n",
    "    region_mask = (labeled_mask == second_largest_label).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(region_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours or len(contours[0]) < 5:\n",
    "        return 1\n",
    "\n",
    "    try:\n",
    "        ellipse = cv2.fitEllipse(contours[0])\n",
    "        (_, (axis1, axis2), _) = ellipse\n",
    "        major_axis = max(axis1, axis2)\n",
    "        minor_axis = min(axis1, axis2)\n",
    "\n",
    "        if major_axis == 0 or minor_axis == 0:\n",
    "            return 1\n",
    "\n",
    "        ratio = minor_axis / major_axis\n",
    "        ratio = np.clip(ratio, 0.0, 1.0)  # Prevent sqrt of negative\n",
    "        eccentricity = np.sqrt(1 - ratio ** 2)\n",
    "        return eccentricity\n",
    "\n",
    "    except cv2.error:\n",
    "        return 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_best_mask(masks, scores):\n",
    "    \"\"\"Select best mask according to criteria, including eccentricity condition.\n",
    "       Returns tuple of (mask, score, eccentricity)\"\"\"\n",
    "    top_indices = np.argsort(scores)[-3:][::-1]  # Top 3 masks by score\n",
    "    top_masks = [masks[i] for i in top_indices]\n",
    "    top_scores = [scores[i] for i in top_indices]\n",
    "\n",
    "    for mask, score in zip(top_masks, top_scores):\n",
    "        mask_area = np.sum(mask)\n",
    "        if mask_area < 40000 and score > 0.02 and has_two_non_overlapping_regions(mask):\n",
    "            ecc = calculate_eccentricity(mask)\n",
    "            if ecc < 0.91:\n",
    "                return mask, score, ecc  # Return eccentricity with override\n",
    "\n",
    "    # Fallback: return best scoring mask with its eccentricity\n",
    "    best_idx = np.argmax(scores)\n",
    "    best_mask = masks[best_idx]\n",
    "    best_ecc = calculate_eccentricity(best_mask)\n",
    "    return best_mask, scores[best_idx], best_ecc\n",
    "\n",
    "\n",
    "# def select_best_mask(masks, scores):\n",
    "#     \"\"\"Select best mask according to criteria, including eccentricity condition.\n",
    "#        Returns tuple of (mask, score, eccentricity)\"\"\"\n",
    "#     top_indices = np.argsort(scores)[-3:][::-1]  # Top 3 masks by score\n",
    "#     top_masks = [masks[i] for i in top_indices]\n",
    "#     top_scores = [scores[i] for i in top_indices]\n",
    "\n",
    "#     # Check all top 3 masks for override conditions\n",
    "#     for mask, score in zip(top_masks, top_scores):\n",
    "#         mask_area = np.sum(mask)\n",
    "#         if mask_area < 40000 and score > 0.02 and has_two_non_overlapping_regions(mask):\n",
    "#             ecc = calculate_eccentricity(mask)\n",
    "#             if ecc < 0.91:\n",
    "#                 return mask, score, ecc  # Return eccentricity with override\n",
    "\n",
    "#     # Fallback: check the highest scoring mask for eccentricity condition\n",
    "#     best_mask = top_masks[0]\n",
    "#     best_score = top_scores[0]\n",
    "#     best_ecc = calculate_eccentricity(best_mask)\n",
    "    \n",
    "#     # Check if highest score mask has problematic characteristics\n",
    "#     if (has_two_non_overlapping_regions(best_mask) and \n",
    "#         best_ecc > 0.91 and \n",
    "#         len(top_masks) > 1):  # Ensure there is a second mask to fall back to\n",
    "#         # Fall back to second highest scoring mask\n",
    "#         second_mask = top_masks[1]\n",
    "#         second_score = top_scores[1]\n",
    "#         second_ecc = calculate_eccentricity(second_mask)\n",
    "#         return second_mask, second_score, second_ecc\n",
    "    \n",
    "#     # Else continue with the highest scoring mask\n",
    "#     return best_mask, best_score, best_ecc\n",
    "    \n",
    "\n",
    "\n",
    "def log_error(subfolder, img_name, error_type, details=\"\"):\n",
    "    \"\"\"Record processing errors to error_log.\"\"\"\n",
    "    error_log.append({\n",
    "        'subfolder': subfolder,\n",
    "        'image_name': img_name,\n",
    "        'error_type': error_type,\n",
    "        'details': str(details)[:200]  # Truncate long error messages\n",
    "    })\n",
    "    #print(f\"! ERROR [{error_type}] {subfolder}/{img_name}: {details}\")\n",
    "\n",
    "def process_images_with_sam2(label, num_images=None):\n",
    "    \"\"\"Process images with comprehensive error handling.\"\"\"\n",
    "    collected = 0\n",
    "    \n",
    "    # Load consolidated CSV once at the beginning\n",
    "    try:\n",
    "        coord_df = pd.read_csv(consolidated_csv)\n",
    "    except Exception as e:\n",
    "        log_error(\"GLOBAL\", \"ALL\", \"CONSOLIDATED_CSV_LOAD_ERROR\", e)\n",
    "        return\n",
    "\n",
    "    valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".tif\"}\n",
    "\n",
    "    for subfolder in sorted(os.listdir(image_dirs[label])):\n",
    "        subfolder_path = os.path.join(image_dirs[label], subfolder)\n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue\n",
    "\n",
    "        images = os.listdir(subfolder_path)\n",
    "        for img_name in tqdm(images, desc=f\"[{label.upper()}] {subfolder}\"):\n",
    "            if num_images is not None and collected >= num_images:\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                # Validate CSV entry - now looking in consolidated file\n",
    "                row = coord_df[coord_df['actual_filename'] == img_name]  # Changed column name\n",
    "                if row.empty:\n",
    "                    log_error(subfolder, img_name, \"MISSING_CSV_ENTRY\")\n",
    "                    continue\n",
    "\n",
    "                row = row.iloc[0]\n",
    "                try:\n",
    "                    lx, ly = int(row['abs_lx']), int(row['abs_ly'])\n",
    "                    rx, ry = int(row['abs_rx']), int(row['abs_ry'])\n",
    "                except (ValueError, KeyError) as e:\n",
    "                    log_error(subfolder, img_name, \"INVALID_COORDINATES\", e)\n",
    "                    continue\n",
    "\n",
    "                # Check file extension before loading\n",
    "                img_ext = os.path.splitext(img_name)[-1].lower()\n",
    "                if img_ext not in valid_extensions:\n",
    "                    log_error(subfolder, img_name, \"INVALID_FILE_EXTENSION\", f\"Extension {img_ext} not supported\")\n",
    "                    continue\n",
    "\n",
    "                # Load and validate image\n",
    "                img_path = os.path.join(subfolder_path, img_name)\n",
    "                try:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"error\")\n",
    "                        image = cv2.imread(img_path)\n",
    "                        if image is None:\n",
    "                            raise AttributeError(\"cv2.imread returned None (possibly unreadable/corrupt)\")\n",
    "                        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                except (UnidentifiedImageError, Warning, cv2.error, TypeError, OSError, AttributeError) as e:\n",
    "                    log_error(subfolder, img_name, \"IMAGE_LOAD_ERROR\", e)\n",
    "                    continue\n",
    "\n",
    "                # Process with SAM2\n",
    "                predictor.set_image(image_rgb)\n",
    "                masks, scores, logits = predictor.predict(\n",
    "                    point_coords=np.array([[lx, ly], [rx, ry]]),\n",
    "                    point_labels=np.array([1, 1]),\n",
    "                    multimask_output=True\n",
    "                )\n",
    "\n",
    "                best_mask, best_score, eccentricity = select_best_mask(masks, scores)\n",
    "                results.append({\n",
    "                    'image_name': img_name,\n",
    "                    'label': label,\n",
    "                    'lx': lx, 'ly': ly,\n",
    "                    'rx': rx, 'ry': ry,\n",
    "                    'mask_area': int(np.sum(best_mask)),\n",
    "                    'mask_score': float(best_score),\n",
    "                    'eccentricity': float(eccentricity),  # New field\n",
    "                    'subfolder': subfolder\n",
    "                })\n",
    "\n",
    "                collected += 1\n",
    "                #print(f\"[{label.upper()}] {subfolder}/{img_name} - Processed\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log_error(subfolder, img_name, \"PROCESSING_ERROR\", e)\n",
    "                continue\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting processing...\")\n",
    "    process_images_with_sam2('blink')\n",
    "    process_images_with_sam2('open')\n",
    "\n",
    "    # Save successful results\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(output_csv, index=False)\n",
    "        print(f\"Saved {len(results)} successful results to {output_csv}\")\n",
    "    else:\n",
    "        print(\"No images processed successfully\")\n",
    "\n",
    "    # Save error log\n",
    "    if error_log:\n",
    "        pd.DataFrame(error_log).to_csv(error_csv, index=False)\n",
    "        print(f\"Saved {len(error_log)} errors to {error_csv}\")\n",
    "    else:\n",
    "        print(\"No errors encountered\")\n",
    "\n",
    "    print(\"Processing complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
